{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYyZMqf0POC4"
   },
   "source": [
    "**16.4.2 Spark DataFrames and Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i3X-rFjSPaTP",
    "outputId": "0492e9f2-dd1f-490f-df2e-4675a5f5a7bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
      "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
      "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
      "Get:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
      "Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
      "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
      "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
      "Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
      "Get:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
      "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [879 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [318 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,332 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [42.7 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,153 kB]\n",
      "Get:20 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [49.2 kB]\n",
      "Fetched 6,064 kB in 5s (1,312 kB/s)\n",
      "Reading package lists... Done\n"
     ]
    }
   ],
   "source": [
    "#Install PySpark packages \n",
    "\n",
    "import os\n",
    "# Find the latest version of spark 3.0  from http://www-us.apache.org/dist/spark/ and enter as the spark version\n",
    "# For example:\n",
    "# spark_version = 'spark-3.0.1'\n",
    "spark_version = 'spark-3.0.1'\n",
    "os.environ['SPARK_VERSION']=spark_version\n",
    "\n",
    "# Install Spark and Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www-us.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set Environment Variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
    "\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"BlueSkyHotelReviews\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DXHC0IfhBaJp"
   },
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrameBasics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hikqNHrv8BEW",
    "outputId": "194b2097-e4b2-4e5e-bced-a45a481a5800"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|               words|\n",
      "+---+--------------------+\n",
      "|  0|Here is our DataF...|\n",
      "|  1|We are making one...|\n",
      "|  2|This will look ve...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe = spark.createDataFrame([(0,\"Here is our DataFrame\"),\n",
    "                                   (1,\"We are making one from screach\"),\n",
    "                                   (2, \"This will look very similar to a Pandas DataFrame\")\n",
    "                                   ],[\"id\", \"words\"])\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y3HN1p8U9Sfg",
    "outputId": "10c5992f-a4a5-42f5-a266-c6cacc8ec2d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   food|price|\n",
      "+-------+-----+\n",
      "|  pizza|    0|\n",
      "|  sushi|   12|\n",
      "|chinese|   10|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in data from S3 Buckets\n",
    "from pyspark import SparkFiles\n",
    "url = \"https://s3.amazonaws.com/dataviz-curriculum/day_1/food.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"food.csv\"), sep=\",\", header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TZpDLCv_9l83",
    "outputId": "c67519df-c408-4d2d-abfa-1e1651e0bc85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- food: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print our schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZhPZsUbI-xK-",
    "outputId": "bed2f968-28e3-47f0-8540-24b448bcee82"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['food', 'price']"
      ]
     },
     "execution_count": 107,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EwbmaYNU-9UB",
    "outputId": "9cef9980-671e-4b98-94cd-038721206d51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, food: string, price: string]"
      ]
     },
     "execution_count": 108,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe our data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wfrDOfOj_R--"
   },
   "outputs": [],
   "source": [
    "# Import struct fields that we can use\n",
    "from pyspark.sql.types import StructField, StringType, IntegerType, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hdZt-ba_DYkM",
    "outputId": "cbfe0260-e3aa-4a8b-d1fd-74e320f1a987"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField(food,StringType,true), StructField(price,IntegerType,true)]"
      ]
     },
     "execution_count": 110,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next we need to create the list of struct fields\n",
    "schema = [StructField(\"food\", StringType(), True), StructField(\"price\", IntegerType(), True),]\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pqVpU3PqOhFZ",
    "outputId": "fe731809-88a5-4ef6-af59-8293af7b9ea3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(food,StringType,true),StructField(price,IntegerType,true)))"
      ]
     },
     "execution_count": 111,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass in our fields\n",
    "final = StructType(fields=schema)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jK80vKP5EBjB",
    "outputId": "08f2dfa1-0811-4fd2-e877-58b69331641c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- food: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read our data with our new schema\n",
    "dataframe = spark.read.csv(SparkFiles.get(\"food.csv\"), schema=final, sep=\",\", header=True)\n",
    "dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wtqhIM-kFQuP",
    "outputId": "d6af603e-5565-4f61-a0ff-3a1edaf3b119"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'price'>"
      ]
     },
     "execution_count": 113,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pLUGsNnAFU2E",
    "outputId": "5d1889ab-e160-4e63-dbbe-39ba61f50144"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 114,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataframe['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LaPu9X4FimV",
    "outputId": "c5a08ee7-1d6f-497d-f984-b71a6e48c2b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[price: int]"
      ]
     },
     "execution_count": 115,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.select('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RkjKXyiAFoc_",
    "outputId": "5e0e63f0-ea48-4086-ac34-7d2398628a51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 116,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataframe.select('price'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KF7SlgQwFtqm",
    "outputId": "ca6d36b5-a4f7-4989-a6f3-0f6652bd1f6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|price|\n",
      "+-----+\n",
      "|    0|\n",
      "|   12|\n",
      "|   10|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.select('price').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3I2BeVyxF2o_",
    "outputId": "2fe13443-ad67-421b-cd7a-391aaac68342"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+\n",
      "|   food|price|newprice|\n",
      "+-------+-----+--------+\n",
      "|  pizza|    0|       0|\n",
      "|  sushi|   12|      12|\n",
      "|chinese|   10|      10|\n",
      "+-------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add new column\n",
    "dataframe.withColumn('newprice', dataframe['price']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R981D1l5HGT0",
    "outputId": "70251e6f-b8d5-4b6e-aa4f-c243b4031c1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|   food|newerprice|\n",
      "+-------+----------+\n",
      "|  pizza|         0|\n",
      "|  sushi|        12|\n",
      "|chinese|        10|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update colomn name\n",
    "dataframe.withColumnRenamed('price', 'newerprice').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4dwthcWTHdNv",
    "outputId": "b4a6159f-56d4-4b89-c513-39c4618da61e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----------+\n",
      "|   food|price|doubleprice|\n",
      "+-------+-----+-----------+\n",
      "|  pizza|    0|          0|\n",
      "|  sushi|   12|         24|\n",
      "|chinese|   10|         20|\n",
      "+-------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Double the price\n",
    "dataframe.withColumn('doubleprice', dataframe['price']*2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YT-YNMXcHzEp",
    "outputId": "cd1c7da7-1f44-4989-fec9-90e0ca474812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------------+\n",
      "|   food|price|add_one_dollar|\n",
      "+-------+-----+--------------+\n",
      "|  pizza|    0|             1|\n",
      "|  sushi|   12|            13|\n",
      "|chinese|   10|            11|\n",
      "+-------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a dollar to the price\n",
    "dataframe.withColumn('add_one_dollar', dataframe['price']+1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KYdec_4EIEP7",
    "outputId": "bbf45c34-ba25-42c5-d406-098824502c99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----------+\n",
      "|   food|price|half_price|\n",
      "+-------+-----+----------+\n",
      "|  pizza|    0|       0.0|\n",
      "|  sushi|   12|       6.0|\n",
      "|chinese|   10|       5.0|\n",
      "+-------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Half the price\n",
    "dataframe.withColumn('half_price',dataframe['price']/2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWx1fBnpPDpV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLINGyslRY1g"
   },
   "source": [
    "**16.4.3 Spark Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJEo-JqQRXtO"
   },
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrameFunctions\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZmpmLaPVkIHI",
    "outputId": "5d97b2d6-b8d1-4287-f24a-09293f821936"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+------+-----+------------------+--------------------+-----------------+------------------+--------------------+\n",
      "|country|         description|         designation|points|price|          province|            region_1|         region_2|           variety|              winery|\n",
      "+-------+--------------------+--------------------+------+-----+------------------+--------------------+-----------------+------------------+--------------------+\n",
      "|     US|This tremendous 1...|   Martha's Vineyard|    96|  235|        California|         Napa Valley|             Napa|Cabernet Sauvignon|               Heitz|\n",
      "|  Spain|Ripe aromas of fi...|Carodorum Selecci...|    96|  110|    Northern Spain|                Toro|             null|     Tinta de Toro|Bodega Carmen Rod...|\n",
      "|     US|Mac Watson honors...|Special Selected ...|    96|   90|        California|      Knights Valley|           Sonoma|   Sauvignon Blanc|            Macauley|\n",
      "|     US|This spent 20 mon...|             Reserve|    96|   65|            Oregon|   Willamette Valley|Willamette Valley|        Pinot Noir|               Ponzi|\n",
      "| France|This is the top w...|         La Br��lade|    95|   66|          Provence|              Bandol|             null|Provence red blend|Domaine de la B̩gude|\n",
      "|  Spain|Deep, dense and p...|           Numanthia|    95|   73|    Northern Spain|                Toro|             null|     Tinta de Toro|           Numanthia|\n",
      "|  Spain|Slightly gritty b...|          San Rom��n|    95|   65|    Northern Spain|                Toro|             null|     Tinta de Toro|            Maurodos|\n",
      "|  Spain|Lush cedary black...|Carodorum �_nico ...|    95|  110|    Northern Spain|                Toro|             null|     Tinta de Toro|Bodega Carmen Rod...|\n",
      "|     US|This re-named vin...|              Silice|    95|   65|            Oregon|  Chehalem Mountains|Willamette Valley|        Pinot Noir|           Bergstr̦m|\n",
      "|     US|The producer sour...|Gap's Crown Vineyard|    95|   60|        California|        Sonoma Coast|           Sonoma|        Pinot Noir|           Blue Farm|\n",
      "|  Italy|Elegance, complex...|  Ronco della Chiesa|    95|   80|Northeastern Italy|              Collio|             null|          Friulano|    Borgo del Tiglio|\n",
      "|     US|From 18-year-old ...|Estate Vineyard W...|    95|   48|            Oregon|        Ribbon Ridge|Willamette Valley|        Pinot Noir|Patricia Green Ce...|\n",
      "|     US|A standout even i...|      Weber Vineyard|    95|   48|            Oregon|        Dundee Hills|Willamette Valley|        Pinot Noir|Patricia Green Ce...|\n",
      "| France|This wine is in p...|Ch̢teau Montus Pr...|    95|   90|  Southwest France|             Madiran|             null|            Tannat|   Vignobles Brumont|\n",
      "|     US|With its sophisti...|      Grace Vineyard|    95|  185|            Oregon|        Dundee Hills|Willamette Valley|        Pinot Noir|      Domaine Serene|\n",
      "|     US|First made in 200...|              Sigrid|    95|   90|            Oregon|   Willamette Valley|Willamette Valley|        Chardonnay|           Bergstr̦m|\n",
      "|     US|This blockbuster,...|     Rainin Vineyard|    95|  325|        California|Diamond Mountain ...|             Napa|Cabernet Sauvignon|                Hall|\n",
      "|  Spain|Nicely oaked blac...|6 A̱os Reserva Pr...|    95|   80|    Northern Spain|    Ribera del Duero|             null|       Tempranillo|            Valduero|\n",
      "| France|Coming from a sev...|       Le Pigeonnier|    95|  290|  Southwest France|              Cahors|             null|            Malbec|  Ch̢teau Lagr̩zette|\n",
      "|     US|This fresh and li...|Gap's Crown Vineyard|    95|   75|        California|        Sonoma Coast|           Sonoma|        Pinot Noir|        Gary Farrell|\n",
      "+-------+--------------------+--------------------+------+-----+------------------+--------------------+-----------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in data from S3 Buckets\n",
    "from pyspark import SparkFiles\n",
    "url =\"https://s3.amazonaws.com/dataviz-curriculum/day_1/wine.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"wine.csv\"), sep=\",\", header=True)\n",
    "# Show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oPsHgc5UnXie",
    "outputId": "5df851fc-edab-45fe-8ed1-69b15bf42875"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[country: string, description: string, designation: string, points: string, price: string, province: string, region_1: string, region_2: string, variety: string, winery: string]"
      ]
     },
     "execution_count": 125,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Order a DataFrame by ascending values\n",
    "df.orderBy(df[\"points\"].desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zdkXuNSnn1Zd",
    "outputId": "78afde1c-cff5-4b35-df26-b836f06fdfd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+------+-----+----------+--------------------+---------------+---------------+--------------------+\n",
      "|country|         description|         designation|points|price|  province|            region_1|       region_2|        variety|              winery|\n",
      "+-------+--------------------+--------------------+------+-----+----------+--------------------+---------------+---------------+--------------------+\n",
      "|  Italy|Here's a ���wow�۝...|            Messorio|    99|  320|   Tuscany|             Toscana|           null|         Merlot|        Le Macchiole|\n",
      "|     US|A stupendous Pino...|Precious Mountain...|    99|   94|California|        Sonoma Coast|         Sonoma|     Pinot Noir|     Williams Selyem|\n",
      "|  Italy|The 2007 Ornellai...|           Ornellaia|    99|  200|   Tuscany|  Bolgheri Superiore|           null|      Red Blend|Tenuta dell'Ornel...|\n",
      "|     US|Depth and texture...|Royal City Stoner...|    99|  140|Washington|Columbia Valley (WA)|Columbia Valley|          Syrah|       Charles Smith|\n",
      "| France|A magnificent Cha...|Dom P̩rignon Oeno...|    99|  385| Champagne|           Champagne|           null|Champagne Blend|     Mo��t & Chandon|\n",
      "+-------+--------------------+--------------------+------+-----+----------+--------------------+---------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# actions\n",
    "df.orderBy(df[\"points\"].desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nPn-MzLYtiCm",
    "outputId": "dd74628a-ad24-4b42-d8ca-8ee1b0c9e358"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|      avg(points)|\n",
      "+-----------------+\n",
      "|87.88834105383143|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import functions\n",
    "from pyspark.sql.functions import avg\n",
    "df.select(avg(\"points\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3CrAh75hv9VO",
    "outputId": "b47f2fee-c696-44dc-96d2-729b2241de98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+------+-----+----------+--------------------+-----------------+--------------+--------------------+\n",
      "| country|         description|designation|points|price|  province|            region_1|         region_2|       variety|              winery|\n",
      "+--------+--------------------+-----------+------+-----+----------+--------------------+-----------------+--------------+--------------------+\n",
      "|Bulgaria|This Bulgarian Ma...|    Bergul̩|    90|   15|  Bulgaria|                null|             null|        Mavrud|        Villa Melnik|\n",
      "|   Spain|Earthy plum and c...|     Amandi|    90|   17|   Galicia|       Ribeira Sacra|             null|       Menc�_a|      Don Bernardino|\n",
      "|      US|There's a lot to ...|       null|    90|   18|California|Russian River Valley|           Sonoma|    Chardonnay|            De Loach|\n",
      "|      US|Massively fruity,...|       null|    91|   19|    Oregon|   Willamette Valley|Willamette Valley|    Pinot Gris|   Trinity Vineyards|\n",
      "|Portugal|It is the ripe da...|    Premium|    91|   15|  Alentejo|                null|             null|Portuguese Red|Adega Cooperativa...|\n",
      "+--------+--------------------+-----------+------+-----+----------+--------------------+-----------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter\n",
    "df.filter(\"price<20\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nBmJqxmDwCqt",
    "outputId": "5010103c-058b-4970-ad10-4c9522e5480e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------------------+-----+\n",
      "|points| country|              winery|price|\n",
      "+------+--------+--------------------+-----+\n",
      "|    90|Bulgaria|        Villa Melnik|   15|\n",
      "|    90|   Spain|      Don Bernardino|   17|\n",
      "|    90|      US|            De Loach|   18|\n",
      "|    91|      US|   Trinity Vineyards|   19|\n",
      "|    91|Portugal|Adega Cooperativa...|   15|\n",
      "+------+--------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter by price on certain columns\n",
    "df.filter(\"price<20\").select(['points','country', 'winery', 'price']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eo9WR280wH36",
    "outputId": "d1231201-d9c8-48e2-e9c2-16791d6899f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+------+-----+----------+--------------------+-----------------+------------------+--------------------+\n",
      "|country|         description|         designation|points|price|  province|            region_1|         region_2|           variety|              winery|\n",
      "+-------+--------------------+--------------------+------+-----+----------+--------------------+-----------------+------------------+--------------------+\n",
      "|     US|This tremendous 1...|   Martha's Vineyard|    96|  235|California|         Napa Valley|             Napa|Cabernet Sauvignon|               Heitz|\n",
      "|     US|Mac Watson honors...|Special Selected ...|    96|   90|California|      Knights Valley|           Sonoma|   Sauvignon Blanc|            Macauley|\n",
      "|     US|This spent 20 mon...|             Reserve|    96|   65|    Oregon|   Willamette Valley|Willamette Valley|        Pinot Noir|               Ponzi|\n",
      "|     US|This re-named vin...|              Silice|    95|   65|    Oregon|  Chehalem Mountains|Willamette Valley|        Pinot Noir|           Bergstr̦m|\n",
      "|     US|The producer sour...|Gap's Crown Vineyard|    95|   60|California|        Sonoma Coast|           Sonoma|        Pinot Noir|           Blue Farm|\n",
      "|     US|From 18-year-old ...|Estate Vineyard W...|    95|   48|    Oregon|        Ribbon Ridge|Willamette Valley|        Pinot Noir|Patricia Green Ce...|\n",
      "|     US|A standout even i...|      Weber Vineyard|    95|   48|    Oregon|        Dundee Hills|Willamette Valley|        Pinot Noir|Patricia Green Ce...|\n",
      "|     US|With its sophisti...|      Grace Vineyard|    95|  185|    Oregon|        Dundee Hills|Willamette Valley|        Pinot Noir|      Domaine Serene|\n",
      "|     US|First made in 200...|              Sigrid|    95|   90|    Oregon|   Willamette Valley|Willamette Valley|        Chardonnay|           Bergstr̦m|\n",
      "|     US|This blockbuster,...|     Rainin Vineyard|    95|  325|California|Diamond Mountain ...|             Napa|Cabernet Sauvignon|                Hall|\n",
      "|     US|This fresh and li...|Gap's Crown Vineyard|    95|   75|California|        Sonoma Coast|           Sonoma|        Pinot Noir|        Gary Farrell|\n",
      "|     US|Heitz has made th...|          Grignolino|    95|   24|California|         Napa Valley|             Napa|              Ros̩|               Heitz|\n",
      "|     US|The apogee of thi...|       Giallo Solare|    95|   60|California|         Edna Valley|    Central Coast|        Chardonnay|    Center of Effort|\n",
      "|     US|San Jose-based pr...|       R-Bar-R Ranch|    95|   45|California|Santa Cruz Mountains|    Central Coast|        Pinot Noir|            Comartin|\n",
      "|     US|Bergstr̦m has mad...|       Shea Vineyard|    94|   62|    Oregon|   Willamette Valley|             null|        Pinot Noir|           Bergstr̦m|\n",
      "|     US|Focused and dense...|             Abetina|    94|  105|    Oregon|   Willamette Valley|Willamette Valley|        Pinot Noir|               Ponzi|\n",
      "|     US|Cranberry, baked ...|     Garys' Vineyard|    94|   60|California|Santa Lucia Highl...|    Central Coast|        Pinot Noir|                Roar|\n",
      "|     US|This standout Roc...|     The Funk Estate|    94|   60|Washington|Walla Walla Valle...|  Columbia Valley|             Syrah|              Saviah|\n",
      "|     US|Steely and perfum...|            Babushka|    90|   37|California|Russian River Valley|           Sonoma|        Chardonnay|            Zepaltas|\n",
      "|     US|The aromas entice...| Conner Lee Vineyard|    90|   42|Washington|Columbia Valley (WA)|  Columbia Valley|        Chardonnay|                Buty|\n",
      "+-------+--------------------+--------------------+------+-----+----------+--------------------+-----------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter on exace stateMB\n",
    "df.filter(df[\"country\"] == \"US\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKlpSAaP1XEl"
   },
   "source": [
    "**16.5.1 Natural Language Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Wa6jbkN0jnC",
    "outputId": "68825af5-7bb0-47ca-9314-1ac8ea3bcc97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
      "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "#Natural Language Toolkit library\n",
    "! pip install nltk\n",
    "! python -m nltk.downloader popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MwpSyBOh0zJe",
    "outputId": "35c1a7b5-598e-4b3b-b4e2-ae9aee69a2c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('enjoy', 'VBP'), ('biking', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('trails', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "# get the PoS tags\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "text = word_tokenize(\"I enjoy biking on the trails\")\n",
    "output = nltk.pos_tag(text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VmFZEn6l4xa0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxszGTNi4xvx"
   },
   "source": [
    "16.6.1 Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eUguC2kM2waU"
   },
   "outputs": [],
   "source": [
    "# import the Tokenizer library from pyspark\n",
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9tPFm4qF3X9O",
    "outputId": "87cf32c8-6cca-478d-8059-cbb70013e161"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            sentence|\n",
      "+---+--------------------+\n",
      "|  0|      Spark is great|\n",
      "|  1|We are learning S...|\n",
      "|  2|Spark is better t...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creat sample DataFrame\n",
    "dataframe =spark.createDataFrame([\n",
    "                                  (0, \"Spark is great\"),\n",
    "                                  (1, \"We are learning Spark\"),\n",
    "                                  (2, \"Spark is better than hadoop no doubt\")\n",
    "], [\"id\", \"sentence\"])\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wqwDzci33a6Z",
    "outputId": "c4778532-e0b6-4e3b-e70a-48ec1d6ded79"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer_d8ef7d8cf7c2"
      ]
     },
     "execution_count": 135,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize sentences\n",
    "tokenizer = Tokenizer (inputCol=\"sentence\", outputCol=\"words\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zFg7ZOsF3jvz",
    "outputId": "d6e8a6b2-d34b-48dd-b292-6d301eb277ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------+--------------------------------------------+\n",
      "|id |sentence                            |words                                       |\n",
      "+---+------------------------------------+--------------------------------------------+\n",
      "|0  |Spark is great                      |[spark, is, great]                          |\n",
      "|1  |We are learning Spark               |[we, are, learning, spark]                  |\n",
      "|2  |Spark is better than hadoop no doubt|[spark, is, better, than, hadoop, no, doubt]|\n",
      "+---+------------------------------------+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform and show DataFrame\n",
    "tokenized_df = tokenizer.transform(dataframe)\n",
    "tokenized_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WIM-R3Ua3p5X"
   },
   "outputs": [],
   "source": [
    "# Creat a fuction to return the length of a list\n",
    "def word_list_length(word_list):\n",
    "  return len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5zqcjzIP3lct"
   },
   "outputs": [],
   "source": [
    "# import the udf function, the col function to select a column to be passed into a function,\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jxa0C8zL4Ap5"
   },
   "outputs": [],
   "source": [
    "# Creat a user defined function\n",
    "count_tokens = udf(word_list_length, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YneAIZmz4HEz",
    "outputId": "08700fae-a5cd-450c-e990-c74aef01a195"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------+--------------------------------------------+------+\n",
      "|id |sentence                            |words                                       |tokens|\n",
      "+---+------------------------------------+--------------------------------------------+------+\n",
      "|0  |Spark is great                      |[spark, is, great]                          |3     |\n",
      "|1  |We are learning Spark               |[we, are, learning, spark]                  |4     |\n",
      "|2  |Spark is better than hadoop no doubt|[spark, is, better, than, hadoop, no, doubt]|7     |\n",
      "+---+------------------------------------+--------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creat our Tokenizer\n",
    "tokenizer = Tokenizer(inputCol = \"sentence\", outputCol = \"words\")\n",
    "# Transform DataFrame\n",
    "tokenized_df = tokenizer.transform(dataframe)\n",
    "# Select the needed columns and don't truncate results\n",
    "tokenized_df.withColumn(\"tokens\", count_tokens(col(\"words\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tST0jNZr5NfA"
   },
   "source": [
    "**16.6.2 Stop Words**\n",
    ": Stop words are words that have little or no linguistic value in NLP.\n",
    "Removing these words from the data can improve the accuracy of the\n",
    "language model because it removes inessential words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h1zNL5yg4UmJ",
    "outputId": "d6fd3d08-66de-4925-81bc-a803c85390a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------+\n",
      "|id |raw                             |\n",
      "+---+--------------------------------+\n",
      "|0  |[Big, data, is, super, powerful]|\n",
      "|1  |[This, is, going, to, be, epic] |\n",
      "+---+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creat DataFrame\n",
    "sentenceData = spark.createDataFrame([\n",
    "                                      (0,[\"Big\", \"data\", \"is\", \"super\", \"powerful\"]),\n",
    "                                      (1,[\"This\", \"is\", \"going\", \"to\", \"be\", \"epic\"])\n",
    "], [\"id\", \"raw\"])\n",
    "sentenceData.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Grx7GEPh4YFo"
   },
   "outputs": [],
   "source": [
    "# Import stop words library\n",
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KVIM0CEH5rm3",
    "outputId": "8f75a9f8-9609-44b3-8720-ab988c091dcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------+----------------------------+\n",
      "|id |raw                             |filtered                    |\n",
      "+---+--------------------------------+----------------------------+\n",
      "|0  |[Big, data, is, super, powerful]|[Big, data, super, powerful]|\n",
      "|1  |[This, is, going, to, be, epic] |[going, epic]               |\n",
      "+---+--------------------------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the Remover\n",
    "remover = StopWordsRemover(inputCol = \"raw\", outputCol = \"filtered\")\n",
    "remover.transform(sentenceData).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTTfOgCG5zzT"
   },
   "source": [
    "**16.6.3 Term Frequency-Inverse Document Frequency Weight**\n",
    ": a statistical weight showing the importance of a word in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VyKkTApS5sN8"
   },
   "outputs": [],
   "source": [
    "# Start Spark sesion\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"TF-IDF\").getOrCreate()\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qlxxVegB6De-",
    "outputId": "d1bb51e9-c190-463e-d7e9-cdf9d62d3b61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      Airline Tweets|\n",
      "+--------------------+\n",
      "|@VirginAmerica pl...|\n",
      "|@VirginAmerica se...|\n",
      "|@VirginAmerica do...|\n",
      "|@VirginAmerica Ar...|\n",
      "|@VirginAmerica aw...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in data from S3 Buckets\n",
    "from pyspark import SparkFiles\n",
    "url =\"https://s3.amazonaws.com/dataviz-curriculum/day_2/airlines.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"airlines.csv\"), sep=\",\", header=True)\n",
    "\n",
    "# show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DRJQJVF_6Fro",
    "outputId": "f0141da4-89d7-4335-a1c2-9808586bac92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|      Airline Tweets|               words|\n",
      "+--------------------+--------------------+\n",
      "|@VirginAmerica pl...|[@virginamerica, ...|\n",
      "|@VirginAmerica se...|[@virginamerica, ...|\n",
      "|@VirginAmerica do...|[@virginamerica, ...|\n",
      "|@VirginAmerica Ar...|[@virginamerica, ...|\n",
      "|@VirginAmerica aw...|[@virginamerica, ...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize DataFrame\n",
    "tokened = Tokenizer(inputCol=\"Airline Tweets\", outputCol=\"words\")\n",
    "tokened_transformed = tokened.transform(df)\n",
    "tokened_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eZwrEQns6UE5",
    "outputId": "8c7678e2-608e-4613-974a-b3d6dd1eb4eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n",
      "|Airline Tweets                                                                                                                         |words                                                                                                                                                          |filtered                                                                                       |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n",
      "|@VirginAmerica plus you've added commercials to the experience... tacky.                                                               |[@virginamerica, plus, you've, added, commercials, to, the, experience..., tacky.]                                                                             |[@virginamerica, plus, added, commercials, experience..., tacky.]                              |\n",
      "|@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing. it's really the only bad thing about flying VA|[@virginamerica, seriously, would, pay, $30, a, flight, for, seats, that, didn't, have, this, playing., it's, really, the, only, bad, thing, about, flying, va]|[@virginamerica, seriously, pay, $30, flight, seats, playing., really, bad, thing, flying, va] |\n",
      "|@VirginAmerica do you miss me? Don't worry we'll be together very soon.                                                                |[@virginamerica, do, you, miss, me?, don't, worry, we'll, be, together, very, soon.]                                                                           |[@virginamerica, miss, me?, worry, together, soon.]                                            |\n",
      "|@VirginAmerica Are the hours of operation for the Club at SFO that are posted online current?                                          |[@virginamerica, are, the, hours, of, operation, for, the, club, at, sfo, that, are, posted, online, current?]                                                 |[@virginamerica, hours, operation, club, sfo, posted, online, current?]                        |\n",
      "|@VirginAmerica awaiting my return phone call, just would prefer to use your online self-service option :(                              |[@virginamerica, awaiting, my, return, phone, call,, just, would, prefer, to, use, your, online, self-service, option, :(]                                     |[@virginamerica, awaiting, return, phone, call,, prefer, use, online, self-service, option, :(]|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "removed_frame = remover.transform(tokened_transformed)\n",
    "removed_frame.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y9-H24LL6Yyw",
    "outputId": "d54e3428-0bf1-47b7-e3c2-a7f8cf79e92b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Airline Tweets                                                                                                                         |words                                                                                                                                                          |filtered                                                                                       |hashedValues                                                                                                                               |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|@VirginAmerica plus you've added commercials to the experience... tacky.                                                               |[@virginamerica, plus, you've, added, commercials, to, the, experience..., tacky.]                                                                             |[@virginamerica, plus, added, commercials, experience..., tacky.]                              |(262144,[1419,99916,168274,171322,186669,256498],[1.0,1.0,1.0,1.0,1.0,1.0])                                                                |\n",
      "|@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing. it's really the only bad thing about flying VA|[@virginamerica, seriously, would, pay, $30, a, flight, for, seats, that, didn't, have, this, playing., it's, really, the, only, bad, thing, about, flying, va]|[@virginamerica, seriously, pay, $30, flight, seats, playing., really, bad, thing, flying, va] |(262144,[30053,44911,70065,94512,99549,145380,166947,171322,178915,229264,237593,239713],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|@VirginAmerica do you miss me? Don't worry we'll be together very soon.                                                                |[@virginamerica, do, you, miss, me?, don't, worry, we'll, be, together, very, soon.]                                                                           |[@virginamerica, miss, me?, worry, together, soon.]                                            |(262144,[107065,117975,147224,171322,200547,232735],[1.0,1.0,1.0,1.0,1.0,1.0])                                                             |\n",
      "|@VirginAmerica Are the hours of operation for the Club at SFO that are posted online current?                                          |[@virginamerica, are, the, hours, of, operation, for, the, club, at, sfo, that, are, posted, online, current?]                                                 |[@virginamerica, hours, operation, club, sfo, posted, online, current?]                        |(262144,[9641,50671,83962,96266,171322,181964,192171,220194],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                            |\n",
      "|@VirginAmerica awaiting my return phone call, just would prefer to use your online self-service option :(                              |[@virginamerica, awaiting, my, return, phone, call,, just, would, prefer, to, use, your, online, self-service, option, :(]                                     |[@virginamerica, awaiting, return, phone, call,, prefer, use, online, self-service, option, :(]|(262144,[6122,50509,50671,67607,98717,121947,128077,171322,225517,234877,261675],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])            |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the hashing term frequency\n",
    "# HashingTF , which converts words to numeric IDs.\n",
    "# The HashingTF function takes an argument for an input column, an output\n",
    "# column, and a numFeature parameter, which specifies the number of buckets for the split words.\n",
    "hashing = HashingTF(inputCol=\"filtered\", outputCol=\"hashedValues\", numFeatures=pow(2,18))\n",
    "\n",
    "# transform into a DF\n",
    "hashed_df = hashing.transform(removed_frame)\n",
    "hashed_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7RnaNhz6L9h",
    "outputId": "4b65d308-0f6d-4cb0-a499-e00d292a335c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|words                                                                                                                                                          |features                                                                                                                                                                                                                                                                                                        |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[@virginamerica, plus, you've, added, commercials, to, the, experience..., tacky.]                                                                             |(262144,[1419,99916,168274,171322,186669,256498],[1.0986122886681098,1.0986122886681098,1.0986122886681098,0.0,1.0986122886681098,1.0986122886681098])                                                                                                                                                          |\n",
      "|[@virginamerica, seriously, would, pay, $30, a, flight, for, seats, that, didn't, have, this, playing., it's, really, the, only, bad, thing, about, flying, va]|(262144,[30053,44911,70065,94512,99549,145380,166947,171322,178915,229264,237593,239713],[1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,0.0,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098])|\n",
      "|[@virginamerica, do, you, miss, me?, don't, worry, we'll, be, together, very, soon.]                                                                           |(262144,[107065,117975,147224,171322,200547,232735],[1.0986122886681098,1.0986122886681098,1.0986122886681098,0.0,1.0986122886681098,1.0986122886681098])                                                                                                                                                       |\n",
      "|[@virginamerica, are, the, hours, of, operation, for, the, club, at, sfo, that, are, posted, online, current?]                                                 |(262144,[9641,50671,83962,96266,171322,181964,192171,220194],[1.0986122886681098,0.6931471805599453,1.0986122886681098,1.0986122886681098,0.0,1.0986122886681098,1.0986122886681098,1.0986122886681098])                                                                                                        |\n",
      "|[@virginamerica, awaiting, my, return, phone, call,, just, would, prefer, to, use, your, online, self-service, option, :(]                                     |(262144,[6122,50509,50671,67607,98717,121947,128077,171322,225517,234877,261675],[1.0986122886681098,1.0986122886681098,0.6931471805599453,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,0.0,1.0986122886681098,1.0986122886681098,1.0986122886681098])                           |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the IDF on the data set\n",
    "# IDF Modeles  will scale the values while down-weighting based on document frequency.\n",
    "idf = IDF(inputCol=\"hashedValues\", outputCol=\"features\")\n",
    "idfModel = idf.fit(hashed_df)\n",
    "rescaledData = idfModel.transform(hashed_df)\n",
    "\n",
    "# Display the DataFrame\n",
    "rescaledData.select(\"words\", \"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FoLM-ajF-BnO"
   },
   "source": [
    "**16.6.4 Pipeline Setup to Run the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5hyP5k0-AvU"
   },
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Yelp_NLP\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4l8S9Ue17cMS",
    "outputId": "e2f08c01-9ab6-4b5b-c37c-c1c73c152731"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|   class|                text|\n",
      "+--------+--------------------+\n",
      "|positive|Wow... Loved this...|\n",
      "|negative|  Crust is not good.|\n",
      "|negative|Not tasty and the...|\n",
      "|positive|Stopped by during...|\n",
      "|positive|The selection on ...|\n",
      "|negative|Now I am getting ...|\n",
      "|negative|Honeslty it didn'...|\n",
      "|negative|The potatoes were...|\n",
      "|positive|The fries were gr...|\n",
      "|positive|      A great touch.|\n",
      "|positive|Service was very ...|\n",
      "|negative|  Would not go back.|\n",
      "|negative|The cashier had n...|\n",
      "|positive|I tried the Cape ...|\n",
      "|negative|I was disgusted b...|\n",
      "|negative|I was shocked bec...|\n",
      "|positive| Highly recommended.|\n",
      "|negative|Waitress was a li...|\n",
      "|negative|This place is not...|\n",
      "|negative|did not like at all.|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in data from S3 Buckets\n",
    "from pyspark import SparkFiles\n",
    "url = \"https://s3.amazonaws.com/dataviz-curriculum/day_2/yelp_reviews.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"yelp_reviews.csv\"), sep=\",\", header=True)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tSUXKOm6_CUA",
    "outputId": "8d0de88c-0dd1-497e-a4d5-b60e8ca1c4df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------+\n",
      "|   class|                text|length|\n",
      "+--------+--------------------+------+\n",
      "|positive|Wow... Loved this...|    24|\n",
      "|negative|  Crust is not good.|    18|\n",
      "|negative|Not tasty and the...|    41|\n",
      "|positive|Stopped by during...|    87|\n",
      "|positive|The selection on ...|    59|\n",
      "|negative|Now I am getting ...|    46|\n",
      "|negative|Honeslty it didn'...|    37|\n",
      "|negative|The potatoes were...|   111|\n",
      "|positive|The fries were gr...|    25|\n",
      "|positive|      A great touch.|    14|\n",
      "|positive|Service was very ...|    24|\n",
      "|negative|  Would not go back.|    18|\n",
      "|negative|The cashier had n...|    99|\n",
      "|positive|I tried the Cape ...|    59|\n",
      "|negative|I was disgusted b...|    62|\n",
      "|negative|I was shocked bec...|    50|\n",
      "|positive| Highly recommended.|    19|\n",
      "|negative|Waitress was a li...|    38|\n",
      "|negative|This place is not...|    51|\n",
      "|negative|did not like at all.|    20|\n",
      "+--------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a new column that uses the length function to create a future feature with the length of each row.\n",
    "# Import functions\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
    "\n",
    "# create a new column that uses the length function to create a future feature with the length of each row.\n",
    "from pyspark.sql.functions import length\n",
    "# Creat a length column to be used as a future feature\n",
    "data_df = df.withColumn('length', length(df['text']))\n",
    "data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s91z8M0D_JQH"
   },
   "outputs": [],
   "source": [
    "# create all the transformations to be applied in our pipeline. \n",
    "# Note that the StringIndexer encodes a string column to a column of table\n",
    "# indexes. Here we are working with positive and negative game reviews, which will be converted to 0 and 1.\n",
    "\n",
    "# Creat all the features to the data set\n",
    "pos_neg_to_num = StringIndexer (inputCol='class', outputCol='label')\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\n",
    "stopremove = StopWordsRemover(inputCol=\"token_text\", outputCol=\"sotp_tokens\")\n",
    "hashingTF = HashingTF(inputCol=\"sotp_tokens\", outputCol=\"hash_token\")\n",
    "idf = IDF(inputCol=\"hash_token\", outputCol='idf_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rlKfW0C3AzqU"
   },
   "outputs": [],
   "source": [
    "# create a feature vector containing the output from the IDFModel (the last stage in the pipeline) and the length.\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector\n",
    "# Creat feature vectors\n",
    "clean_up = VectorAssembler(inputCols=[\"idf_token\", \"length\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fl6-UAH1BBe0"
   },
   "outputs": [],
   "source": [
    "# We'll import the pipeline from pyspark.ml , and then store a list of the stages created  earlier.\n",
    "# Creat and run a data procession Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "data_prep_pipeline = Pipeline(stages=[pos_neg_to_num, tokenizer, stopremove, hashingTF, idf, clean_up])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xPg8PQgSLVS"
   },
   "source": [
    "**16.6.5 Run the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wqG491peE05X",
    "outputId": "db0a4764-eaf5-467b-f641-52b800985c2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  1.0|(262145,[177414,2...|\n",
      "|  0.0|(262145,[49815,23...|\n",
      "|  0.0|(262145,[109649,1...|\n",
      "|  1.0|(262145,[53101,68...|\n",
      "|  1.0|(262145,[15370,77...|\n",
      "|  0.0|(262145,[98142,13...|\n",
      "|  0.0|(262145,[59172,22...|\n",
      "|  0.0|(262145,[63420,85...|\n",
      "|  1.0|(262145,[53777,17...|\n",
      "|  1.0|(262145,[221827,2...|\n",
      "|  1.0|(262145,[43756,22...|\n",
      "|  0.0|(262145,[127310,1...|\n",
      "|  0.0|(262145,[407,3153...|\n",
      "|  1.0|(262145,[18098,93...|\n",
      "|  0.0|(262145,[23071,12...|\n",
      "|  0.0|(262145,[129941,1...|\n",
      "|  1.0|(262145,[19633,21...|\n",
      "|  0.0|(262145,[27707,65...|\n",
      "|  0.0|(262145,[20891,27...|\n",
      "|  0.0|(262145,[8287,208...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit and transform the pipeline\n",
    "cleaner = data_prep_pipeline.fit(data_df)\n",
    "cleaned = cleaner.transform(data_df)\n",
    "cleaned.select([\"label\", \"features\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJpRDRvFFYXi"
   },
   "outputs": [],
   "source": [
    "# split the data into a training set and a testing set, run the following code:\n",
    "# So 70% to training and 30% to testing. \n",
    "# The second number supplied is called a seed. The seed number here, 21, is arbitrary.\n",
    "# Break data down into a training set and a testing set\n",
    "training, testing = cleaned.randomSplit([0.7, 0.3], 21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D2sG0EMJFQDH",
    "outputId": "5e0e08c0-da25-42af-eba5-97078756b0e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|   class|                text|length|label|          token_text|         sotp_tokens|          hash_token|           idf_token|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|negative|\"The burger... I ...|    86|  0.0|[\"the, burger...,...|[\"the, burger...,...|(262144,[20298,21...|(262144,[20298,21...|(262145,[20298,21...|[-820.60780566975...|[0.99999999999995...|       0.0|\n",
      "|negative|              #NAME?|     6|  0.0|            [#name?]|            [#name?]|(262144,[197050],...|(262144,[197050],...|(262145,[197050,2...|[-73.489435340867...|[0.07515735596910...|       1.0|\n",
      "|negative|After I pulled up...|    83|  0.0|[after, i, pulled...|[pulled, car, wai...|(262144,[65645,71...|(262144,[65645,71...|(262145,[65645,71...|[-620.40646705112...|[1.0,1.9205984091...|       0.0|\n",
      "|negative|Also, I feel like...|    58|  0.0|[also,, i, feel, ...|[also,, feel, lik...|(262144,[61899,66...|(262144,[61899,66...|(262145,[61899,66...|[-528.59562125515...|[0.99999999994873...|       0.0|\n",
      "|negative|Anyway, I do not ...|    44|  0.0|[anyway,, i, do, ...|[anyway,, think, ...|(262144,[132270,1...|(262144,[132270,1...|(262145,[132270,1...|[-334.09599709326...|[0.99999999994185...|       0.0|\n",
      "+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ML model we'll use is Naive Bayes, which we'll import and then fit the model using the training dataset\n",
    "# Naive Bayes is a group of classifier algorithms based on Bayes' theorem.\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "# Creat a Naivy Bayes model and fit traning data\n",
    "nb = NaiveBayes()\n",
    "predictor = nb.fit(training)\n",
    "\n",
    "# Transform the model with the testing data\n",
    "# prediction column will indicate with a 1.0 if the model thinks this\n",
    "# review is negative and 0.0 if it thinks it's positive. Future data sets can now\n",
    "# be run with this model and determine whether a review was positive or\n",
    "# negative without having already supplied in the data.\n",
    "\n",
    "test_results = predictor.transform(testing)\n",
    "test_results.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hlnq9BJrUaEO",
    "outputId": "7b15bf6e-4aeb-44f5-fd6d-8f8157f35f00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting reviews was: 0.700298\n"
     ]
    }
   ],
   "source": [
    "# The last step is to import the BinaryClassificationEvaluator , which\n",
    "# will display how accurate our model is in determining if a review with be\n",
    "# positive or negative based solely on the text within a review.\n",
    "\n",
    "# The BinaryClassificationEvaluator uses two arguments, labelCol and rawPredictionCol.\n",
    "# The labelCol takes the labels which were the result of using StringIndexer to convert our positive and negative strings to\n",
    "# integers. The rawPredictionCol takes in numerical predictions from the\n",
    "# output of running the Naive Bayes model.\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "acc_eval = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\")\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print(\"Accuracy of model at predicting reviews was: %f\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VaOgxVD8ZM4L"
   },
   "source": [
    "**16.9.1 PySpark ETL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZrivI-xCWSuf",
    "outputId": "3d4314f6-542e-4d6e-918c-9fbd2dcf2699"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL transformed to HTTPS due to an HSTS policy\n",
      "--2021-01-27 05:23:41--  https://jdbc.postgresql.org/download/postgresql-42.2.16.jar\n",
      "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
      "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1002883 (979K) [application/java-archive]\n",
      "Saving to: ‘postgresql-42.2.16.jar.1’\n",
      "\n",
      "postgresql-42.2.16. 100%[===================>] 979.38K  1.21MB/s    in 0.8s    \n",
      "\n",
      "2021-01-27 05:23:43 (1.21 MB/s) - ‘postgresql-42.2.16.jar.1’ saved [1002883/1002883]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://jdbc.postgresql.org/download/postgresql-42.2.16.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7og-jm73ZLfN"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"CloudETL\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.16.jar\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b14fO_gcZYcU",
    "outputId": "7c4bc27d-f242-4c8c-a084-bef53090d5c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+-----------+--------------------+--------------------+------------+\n",
      "| id|first_name|last_name|active_user|      street_address|               state|    username|\n",
      "+---+----------+---------+-----------+--------------------+--------------------+------------+\n",
      "|  1|    Cletus|  Lithcow|      false| 78309 Riverside Way|            Virginia|   ibearham0|\n",
      "|  2|       Caz|   Felgat|      false| 83 Hazelcrest Place|             Alabama|    wwaller1|\n",
      "|  3|     Kerri|  Crowson|      false|      112 Eliot Pass|      North Carolina|   ichesnut2|\n",
      "|  4|   Freddie|    Caghy|      false|     15 Merchant Way|            New York|     tsnarr3|\n",
      "|  5|   Sadella|    Deuss|      false|    079 Acker Avenue|           Tennessee|   fwherrit4|\n",
      "|  6|    Fraser|  Korneev|       true|  76084 Novick Court|           Minnesota|  fstappard5|\n",
      "|  7|    Demott|   Rapson|       true|    86320 Dahle Park|District of Columbia|  lhambling6|\n",
      "|  8|    Robert|    Poile|      false|1540 Manitowish Hill|             Georgia|      drude7|\n",
      "|  9|    Nollie|     null|       true|       4 Katie Court|                Ohio|   bspawton8|\n",
      "| 10|   Merilyn| Frascone|      false|     387 Duke Street|                Ohio| rmackeller9|\n",
      "| 11|    Rickie| Tredwell|      false|  04 Monterey Center|            Missouri| cdennerleya|\n",
      "| 12|  Charmane| Connerry|      false|    0 Larry Junction|             Florida|    gsarfasb|\n",
      "| 13|     Nerti|   Kerins|      false|    68 Portage Trail|          California| mpichefordc|\n",
      "| 14|      Bart|     null|      false|    8 Homewood Court|District of Columbia|     bingryd|\n",
      "| 15|   Sadella|    Jaram|       true|7528 Waxwing Terrace|         Connecticut|   wheinerte|\n",
      "| 16|     Dicky|  Runnett|      false|  1793 Delaware Park|             Florida|    mdrewetf|\n",
      "| 17|    Hewitt|  Trammel|       true|    2455 Corry Alley|      North Carolina|droughsedgeg|\n",
      "| 18|  Gilligan|     Boys|      false|       2 Raven Court|             Florida|     abaakeh|\n",
      "| 19|       Ted|  Knowlys|       true|      31 South Drive|                Ohio|   ydudeniei|\n",
      "| 20|      Darb|   Carrel|      false|406 Park Meadow C...|           Minnesota|   ckermittj|\n",
      "+---+----------+---------+-----------+--------------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in Data from S3 Buckets\n",
    "from pyspark import SparkFiles\n",
    "url =\"https://sibbuckettest.s3.us-east-2.amazonaws.com/user_data.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "user_data_df = spark.read.csv(SparkFiles.get(\"user_data.csv\"), sep=\",\", header=True, inferSchema=True)\n",
    "\n",
    "# Show DataFrame\n",
    "user_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VzhB5nwGZrBS",
    "outputId": "ce5eea47-5f5f-4dee-c7a1-ac8c00ecc09f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|billing_id|    username|        cc_encrypted|\n",
      "+----------+------------+--------------------+\n",
      "|         1|   ibearham0|a799fcafe47d7fb19...|\n",
      "|         2|    wwaller1|a799fcafe47d7fb19...|\n",
      "|         3|   ichesnut2|a799fcafe47d7fb19...|\n",
      "|         4|     tsnarr3|a799fcafe47d7fb19...|\n",
      "|         5|   fwherrit4|a799fcafe47d7fb19...|\n",
      "|         6|  fstappard5|a799fcafe47d7fb19...|\n",
      "|         7|  lhambling6|a799fcafe47d7fb19...|\n",
      "|         8|      drude7|a799fcafe47d7fb19...|\n",
      "|         9|   bspawton8|a799fcafe47d7fb19...|\n",
      "|        10| rmackeller9|a799fcafe47d7fb19...|\n",
      "|        11| cdennerleya|a799fcafe47d7fb19...|\n",
      "|        12|    gsarfasb|a799fcafe47d7fb19...|\n",
      "|        13| mpichefordc|a799fcafe47d7fb19...|\n",
      "|        14|     bingryd|a799fcafe47d7fb19...|\n",
      "|        15|   wheinerte|a799fcafe47d7fb19...|\n",
      "|        16|    mdrewetf|a799fcafe47d7fb19...|\n",
      "|        17|droughsedgeg|a799fcafe47d7fb19...|\n",
      "|        18|     abaakeh|a799fcafe47d7fb19...|\n",
      "|        19|   ydudeniei|a799fcafe47d7fb19...|\n",
      "|        20|   ckermittj|a799fcafe47d7fb19...|\n",
      "+----------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url =\"https://sibbuckettest.s3.us-east-2.amazonaws.com/user_payment.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "user_payment_df = spark.read.csv(SparkFiles.get(\"user_payment.csv\"), sep=\",\", header=True, inferSchema=True)\n",
    "# Show DataFrame\n",
    "user_payment_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dcat550zaJkc"
   },
   "source": [
    "**Transform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iWSJwGRrZ9-v",
    "outputId": "b64b72a7-6cb3-4d0d-b549-bf071f3e4614"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+----------+---------+-----------+--------------------+--------------------+----------+--------------------+\n",
      "|    username| id|first_name|last_name|active_user|      street_address|               state|billing_id|        cc_encrypted|\n",
      "+------------+---+----------+---------+-----------+--------------------+--------------------+----------+--------------------+\n",
      "|   ibearham0|  1|    Cletus|  Lithcow|      false| 78309 Riverside Way|            Virginia|         1|a799fcafe47d7fb19...|\n",
      "|    wwaller1|  2|       Caz|   Felgat|      false| 83 Hazelcrest Place|             Alabama|         2|a799fcafe47d7fb19...|\n",
      "|   ichesnut2|  3|     Kerri|  Crowson|      false|      112 Eliot Pass|      North Carolina|         3|a799fcafe47d7fb19...|\n",
      "|     tsnarr3|  4|   Freddie|    Caghy|      false|     15 Merchant Way|            New York|         4|a799fcafe47d7fb19...|\n",
      "|   fwherrit4|  5|   Sadella|    Deuss|      false|    079 Acker Avenue|           Tennessee|         5|a799fcafe47d7fb19...|\n",
      "|  fstappard5|  6|    Fraser|  Korneev|       true|  76084 Novick Court|           Minnesota|         6|a799fcafe47d7fb19...|\n",
      "|  lhambling6|  7|    Demott|   Rapson|       true|    86320 Dahle Park|District of Columbia|         7|a799fcafe47d7fb19...|\n",
      "|      drude7|  8|    Robert|    Poile|      false|1540 Manitowish Hill|             Georgia|         8|a799fcafe47d7fb19...|\n",
      "|   bspawton8|  9|    Nollie|     null|       true|       4 Katie Court|                Ohio|         9|a799fcafe47d7fb19...|\n",
      "| rmackeller9| 10|   Merilyn| Frascone|      false|     387 Duke Street|                Ohio|        10|a799fcafe47d7fb19...|\n",
      "| cdennerleya| 11|    Rickie| Tredwell|      false|  04 Monterey Center|            Missouri|        11|a799fcafe47d7fb19...|\n",
      "|    gsarfasb| 12|  Charmane| Connerry|      false|    0 Larry Junction|             Florida|        12|a799fcafe47d7fb19...|\n",
      "| mpichefordc| 13|     Nerti|   Kerins|      false|    68 Portage Trail|          California|        13|a799fcafe47d7fb19...|\n",
      "|     bingryd| 14|      Bart|     null|      false|    8 Homewood Court|District of Columbia|        14|a799fcafe47d7fb19...|\n",
      "|   wheinerte| 15|   Sadella|    Jaram|       true|7528 Waxwing Terrace|         Connecticut|        15|a799fcafe47d7fb19...|\n",
      "|    mdrewetf| 16|     Dicky|  Runnett|      false|  1793 Delaware Park|             Florida|        16|a799fcafe47d7fb19...|\n",
      "|droughsedgeg| 17|    Hewitt|  Trammel|       true|    2455 Corry Alley|      North Carolina|        17|a799fcafe47d7fb19...|\n",
      "|     abaakeh| 18|  Gilligan|     Boys|      false|       2 Raven Court|             Florida|        18|a799fcafe47d7fb19...|\n",
      "|   ydudeniei| 19|       Ted|  Knowlys|       true|      31 South Drive|                Ohio|        19|a799fcafe47d7fb19...|\n",
      "|   ckermittj| 20|      Darb|   Carrel|      false|406 Park Meadow C...|           Minnesota|        20|a799fcafe47d7fb19...|\n",
      "+------------+---+----------+---------+-----------+--------------------+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join the two tables\n",
    "joined_df = user_data_df.join(user_payment_df, on =\"username\", how=\"inner\")\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jkhu-gzSaQ_t",
    "outputId": "35a57eae-d26f-4d88-a600-da4f997afb96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+----------+---------+-----------+--------------------+--------------------+----------+--------------------+\n",
      "|    username| id|first_name|last_name|active_user|      street_address|               state|billing_id|        cc_encrypted|\n",
      "+------------+---+----------+---------+-----------+--------------------+--------------------+----------+--------------------+\n",
      "|   ibearham0|  1|    Cletus|  Lithcow|      false| 78309 Riverside Way|            Virginia|         1|a799fcafe47d7fb19...|\n",
      "|    wwaller1|  2|       Caz|   Felgat|      false| 83 Hazelcrest Place|             Alabama|         2|a799fcafe47d7fb19...|\n",
      "|   ichesnut2|  3|     Kerri|  Crowson|      false|      112 Eliot Pass|      North Carolina|         3|a799fcafe47d7fb19...|\n",
      "|     tsnarr3|  4|   Freddie|    Caghy|      false|     15 Merchant Way|            New York|         4|a799fcafe47d7fb19...|\n",
      "|   fwherrit4|  5|   Sadella|    Deuss|      false|    079 Acker Avenue|           Tennessee|         5|a799fcafe47d7fb19...|\n",
      "|  fstappard5|  6|    Fraser|  Korneev|       true|  76084 Novick Court|           Minnesota|         6|a799fcafe47d7fb19...|\n",
      "|  lhambling6|  7|    Demott|   Rapson|       true|    86320 Dahle Park|District of Columbia|         7|a799fcafe47d7fb19...|\n",
      "|      drude7|  8|    Robert|    Poile|      false|1540 Manitowish Hill|             Georgia|         8|a799fcafe47d7fb19...|\n",
      "| rmackeller9| 10|   Merilyn| Frascone|      false|     387 Duke Street|                Ohio|        10|a799fcafe47d7fb19...|\n",
      "| cdennerleya| 11|    Rickie| Tredwell|      false|  04 Monterey Center|            Missouri|        11|a799fcafe47d7fb19...|\n",
      "|    gsarfasb| 12|  Charmane| Connerry|      false|    0 Larry Junction|             Florida|        12|a799fcafe47d7fb19...|\n",
      "| mpichefordc| 13|     Nerti|   Kerins|      false|    68 Portage Trail|          California|        13|a799fcafe47d7fb19...|\n",
      "|   wheinerte| 15|   Sadella|    Jaram|       true|7528 Waxwing Terrace|         Connecticut|        15|a799fcafe47d7fb19...|\n",
      "|    mdrewetf| 16|     Dicky|  Runnett|      false|  1793 Delaware Park|             Florida|        16|a799fcafe47d7fb19...|\n",
      "|droughsedgeg| 17|    Hewitt|  Trammel|       true|    2455 Corry Alley|      North Carolina|        17|a799fcafe47d7fb19...|\n",
      "|     abaakeh| 18|  Gilligan|     Boys|      false|       2 Raven Court|             Florida|        18|a799fcafe47d7fb19...|\n",
      "|   ydudeniei| 19|       Ted|  Knowlys|       true|      31 South Drive|                Ohio|        19|a799fcafe47d7fb19...|\n",
      "|   ckermittj| 20|      Darb|   Carrel|      false|406 Park Meadow C...|           Minnesota|        20|a799fcafe47d7fb19...|\n",
      "|     ipowisk| 21|   Diandra|Cancellor|      false|      1 Fisk Parkway|      North Carolina|        21|a799fcafe47d7fb19...|\n",
      "|    dtaltonl| 22|    Ulrika| Itzhayek|      false|  890 Lakewood Alley|          California|        22|a799fcafe47d7fb19...|\n",
      "+------------+---+----------+---------+-----------+--------------------+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop any rows with null\n",
    "dropna_df = joined_df.dropna()\n",
    "dropna_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b8I-OSPaVvm",
    "outputId": "959669ed-3590-48a7-c237-34914ba04e4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+----------+-----------+-----------+--------------------+--------------------+----------+--------------------+\n",
      "|     username| id|first_name|  last_name|active_user|      street_address|               state|billing_id|        cc_encrypted|\n",
      "+-------------+---+----------+-----------+-----------+--------------------+--------------------+----------+--------------------+\n",
      "|   fstappard5|  6|    Fraser|    Korneev|       true|  76084 Novick Court|           Minnesota|         6|a799fcafe47d7fb19...|\n",
      "|   lhambling6|  7|    Demott|     Rapson|       true|    86320 Dahle Park|District of Columbia|         7|a799fcafe47d7fb19...|\n",
      "|    wheinerte| 15|   Sadella|      Jaram|       true|7528 Waxwing Terrace|         Connecticut|        15|a799fcafe47d7fb19...|\n",
      "| droughsedgeg| 17|    Hewitt|    Trammel|       true|    2455 Corry Alley|      North Carolina|        17|a799fcafe47d7fb19...|\n",
      "|    ydudeniei| 19|       Ted|    Knowlys|       true|      31 South Drive|                Ohio|        19|a799fcafe47d7fb19...|\n",
      "|     fmyttonm| 23|  Annmarie|     Lafond|       true|     35 Oriole Place|             Georgia|        23|a799fcafe47d7fb19...|\n",
      "|   bfletcherr| 28|      Toma|     Sokell|       true|39641 Eggendart Hill|            Maryland|        28|a799fcafe47d7fb19...|\n",
      "|     gturleyt| 30|       Ram|    Lefever|       true|   9969 Laurel Alley|               Texas|        30|a799fcafe47d7fb19...|\n",
      "|    calyukinu| 31|    Raddie|    Heindle|       true|   811 Talmadge Road|                Ohio|        31|a799fcafe47d7fb19...|\n",
      "| ckleinlererw| 33|    Wallie|       Caws|       true|   9999 Kenwood Pass|              Oregon|        33|a799fcafe47d7fb19...|\n",
      "|  pshanklandx| 34|    Derril|Varfolomeev|       true|     4 Jenifer Court|             Florida|        34|a799fcafe47d7fb19...|\n",
      "|    enelane12| 39|     Kelcy|     Wheway|       true|93207 Morningstar...|             Florida|        39|a799fcafe47d7fb19...|\n",
      "|    sfollet13| 40|    Dorree|    Rookeby|       true|       2 Troy Circle|          California|        40|a799fcafe47d7fb19...|\n",
      "|      mtesh14| 41|    Martyn|       Tott|       true|       728 Muir Lane|             Florida|        41|a799fcafe47d7fb19...|\n",
      "|   tseyfart16| 43|     Cally|      Thody|       true|   1 Graceland Plaza|             Florida|        43|a799fcafe47d7fb19...|\n",
      "|   hfarrier18| 45|       Ted|   Pittaway|       true|767 Little Fleur ...|      North Carolina|        45|a799fcafe47d7fb19...|\n",
      "|     nabbie1b| 48|      Fifi|    Lidgley|       true|6744 Sutherland Road|      South Carolina|        48|a799fcafe47d7fb19...|\n",
      "|  ystadding1d| 50|    Ashely|     O'Hern|       true|   929 Scoville Park|             Florida|        50|a799fcafe47d7fb19...|\n",
      "|hhallgalley1g| 53|   Diannne|Osbaldeston|       true|        0 Mesta Pass|           Tennessee|        53|a799fcafe47d7fb19...|\n",
      "|   ageaveny1n| 60|     Sonny|     Jeskin|       true| 50 Sutherland Drive|       Massachusetts|        60|a799fcafe47d7fb19...|\n",
      "+-------------+---+----------+-----------+-----------+--------------------+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter for active users\n",
    "# Load in a sql function to use columns\n",
    "from pyspark.sql.functions import col \n",
    "# Filter for only columns with active users\n",
    "cleaned_df = dropna_df.filter (col(\"active_user\") == True)\n",
    "cleaned_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i4q6-FocaaM4",
    "outputId": "c81316f5-f849-4cc5-f855-3d9270a58c2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+-------------+\n",
      "| id|first_name|  last_name|     username|\n",
      "+---+----------+-----------+-------------+\n",
      "|  6|    Fraser|    Korneev|   fstappard5|\n",
      "|  7|    Demott|     Rapson|   lhambling6|\n",
      "| 15|   Sadella|      Jaram|    wheinerte|\n",
      "| 17|    Hewitt|    Trammel| droughsedgeg|\n",
      "| 19|       Ted|    Knowlys|    ydudeniei|\n",
      "| 23|  Annmarie|     Lafond|     fmyttonm|\n",
      "| 28|      Toma|     Sokell|   bfletcherr|\n",
      "| 30|       Ram|    Lefever|     gturleyt|\n",
      "| 31|    Raddie|    Heindle|    calyukinu|\n",
      "| 33|    Wallie|       Caws| ckleinlererw|\n",
      "| 34|    Derril|Varfolomeev|  pshanklandx|\n",
      "| 39|     Kelcy|     Wheway|    enelane12|\n",
      "| 40|    Dorree|    Rookeby|    sfollet13|\n",
      "| 41|    Martyn|       Tott|      mtesh14|\n",
      "| 43|     Cally|      Thody|   tseyfart16|\n",
      "| 45|       Ted|   Pittaway|   hfarrier18|\n",
      "| 48|      Fifi|    Lidgley|     nabbie1b|\n",
      "| 50|    Ashely|     O'Hern|  ystadding1d|\n",
      "| 53|   Diannne|Osbaldeston|hhallgalley1g|\n",
      "| 60|     Sonny|     Jeskin|   ageaveny1n|\n",
      "+---+----------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creat user dataframe to match active_user table\n",
    "clean_user_df = cleaned_df.select([\"id\", \"first_name\", \"last_name\", \"username\"])\n",
    "clean_user_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gYQw0nqpbRnv",
    "outputId": "b86cdf1d-1a75-433a-e78d-eef571bcd947"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+-------------+\n",
      "|billing_id|      street_address|               state|     username|\n",
      "+----------+--------------------+--------------------+-------------+\n",
      "|         6|  76084 Novick Court|           Minnesota|   fstappard5|\n",
      "|         7|    86320 Dahle Park|District of Columbia|   lhambling6|\n",
      "|        15|7528 Waxwing Terrace|         Connecticut|    wheinerte|\n",
      "|        17|    2455 Corry Alley|      North Carolina| droughsedgeg|\n",
      "|        19|      31 South Drive|                Ohio|    ydudeniei|\n",
      "|        23|     35 Oriole Place|             Georgia|     fmyttonm|\n",
      "|        28|39641 Eggendart Hill|            Maryland|   bfletcherr|\n",
      "|        30|   9969 Laurel Alley|               Texas|     gturleyt|\n",
      "|        31|   811 Talmadge Road|                Ohio|    calyukinu|\n",
      "|        33|   9999 Kenwood Pass|              Oregon| ckleinlererw|\n",
      "|        34|     4 Jenifer Court|             Florida|  pshanklandx|\n",
      "|        39|93207 Morningstar...|             Florida|    enelane12|\n",
      "|        40|       2 Troy Circle|          California|    sfollet13|\n",
      "|        41|       728 Muir Lane|             Florida|      mtesh14|\n",
      "|        43|   1 Graceland Plaza|             Florida|   tseyfart16|\n",
      "|        45|767 Little Fleur ...|      North Carolina|   hfarrier18|\n",
      "|        48|6744 Sutherland Road|      South Carolina|     nabbie1b|\n",
      "|        50|   929 Scoville Park|             Florida|  ystadding1d|\n",
      "|        53|        0 Mesta Pass|           Tennessee|hhallgalley1g|\n",
      "|        60| 50 Sutherland Drive|       Massachusetts|   ageaveny1n|\n",
      "+----------+--------------------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to match the payment_info table\n",
    "clean_billing_df = cleaned_df.select([\"billing_id\", \"street_address\", \"state\", \"username\"])\n",
    "clean_billing_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jo-Xrn_mbVzt",
    "outputId": "d31ee99b-8870-47b7-8477-2caf2edd1778"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|billing_id|        cc_encrypted|\n",
      "+----------+--------------------+\n",
      "|         6|a799fcafe47d7fb19...|\n",
      "|         7|a799fcafe47d7fb19...|\n",
      "|        15|a799fcafe47d7fb19...|\n",
      "|        17|a799fcafe47d7fb19...|\n",
      "|        19|a799fcafe47d7fb19...|\n",
      "|        23|a799fcafe47d7fb19...|\n",
      "|        28|a799fcafe47d7fb19...|\n",
      "|        30|a799fcafe47d7fb19...|\n",
      "|        31|a799fcafe47d7fb19...|\n",
      "|        33|a799fcafe47d7fb19...|\n",
      "|        34|a799fcafe47d7fb19...|\n",
      "|        39|a799fcafe47d7fb19...|\n",
      "|        40|a799fcafe47d7fb19...|\n",
      "|        41|a799fcafe47d7fb19...|\n",
      "|        43|a799fcafe47d7fb19...|\n",
      "|        45|a799fcafe47d7fb19...|\n",
      "|        48|a799fcafe47d7fb19...|\n",
      "|        50|a799fcafe47d7fb19...|\n",
      "|        53|a799fcafe47d7fb19...|\n",
      "|        60|a799fcafe47d7fb19...|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to match the payment_info table\n",
    "clean_payment_df = cleaned_df.select([\"billing_id\", \"cc_encrypted\"])\n",
    "clean_payment_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAPXPQjGbges"
   },
   "source": [
    "**Load:**\n",
    "final step is to get our transformed raw data into our database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FHFHHLWLbbXL"
   },
   "outputs": [],
   "source": [
    "# Configure settings for RDS\n",
    "mode = \"append\"\n",
    "jdbc_url=\"jdbc:postgresql://amazonuserdb.ckl1urlayos2.us-east-2.rds.amazonaws.com:5432/PracticeDB\"\n",
    "config = {\"user\": \"postgres\",\n",
    "          \"password\": \"hwmqNEtVfE30cMEcDTLu\",\n",
    "          \"driver\":\"org.postgresql.Driver\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpGRgeF0zgo1"
   },
   "outputs": [],
   "source": [
    "clean_user_df.write.jdbc(url=jdbc_url, table='active_user', mode=mode, properties=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3qD27fs2984"
   },
   "outputs": [],
   "source": [
    "clean_billing_df.write.jdbc(url=jdbc_url, table='billing_info', mode=mode, properties=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6H2-QST2-KV"
   },
   "outputs": [],
   "source": [
    "clean_payment_df.write.jdbc(url=jdbc_url, table='payment_info', mode=mode, properties=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jqvEnQeS2-of"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "16.4.2_Spark_Dataframe.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
